"""
ç»Ÿä¸€Embeddingç®¡ç†å™¨ - æ”¯æŒæœ¬åœ°/äº‘ç«¯/æ··åˆæ¨¡å¼
Unified Embedding Manager - Supporting Local/Cloud/Hybrid Modes
"""

import logging
from typing import List, Optional, Union
import numpy as np

from style_rag.core.embedding_config import EmbeddingConfig, EmbeddingProvider

logger = logging.getLogger(__name__)


class EmbeddingManager:
    """
    ç»Ÿä¸€çš„Embeddingç®¡ç†å™¨ - æ”¯æŒæœ¬åœ°/äº‘ç«¯/æ··åˆ
    Unified Embedding Manager - Supporting Local/Cloud/Hybrid
    """
    
    def __init__(self, config: Optional[EmbeddingConfig] = None):
        """
        åˆå§‹åŒ–Embeddingç®¡ç†å™¨
        
        Args:
            config: Embeddingé…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æœ¬åœ°é…ç½®
        """
        self.config = config or EmbeddingConfig()
        self._local_model = None
        self._gguf_model = None  # GGUFæ¨¡å‹å®ä¾‹
        self._api_client = None
        self._api_model = None
        self._fallback_model = None
        self._dimension = None
        self._initialize()
    
    def _initialize(self):
        """åˆå§‹åŒ–Embeddingåç«¯ / Initialize embedding backend"""
        provider = self.config.provider
        
        if provider == EmbeddingProvider.LOCAL:
            self._init_local_model()
        elif provider == EmbeddingProvider.LOCAL_GGUF:
            self._init_gguf_model()
        elif provider == EmbeddingProvider.LM_STUDIO:
            self._init_openai_compatible(
                base_url=self.config.lm_studio_url,
                api_key="not-needed",
                model=self.config.lm_studio_model
            )
        elif provider == EmbeddingProvider.OLLAMA:
            self._init_openai_compatible(
                base_url=f"{self.config.ollama_url}/v1",
                api_key="ollama",
                model=self.config.ollama_model
            )
        elif provider == EmbeddingProvider.ZENMUX:
            self._init_openai_compatible(
                base_url=self.config.zenmux_base_url,
                api_key=self.config.api_key or "zenmux-key",
                model=self.config.zenmux_model
            )
        elif provider == EmbeddingProvider.OPENROUTER:
            self._init_openai_compatible(
                base_url=self.config.openrouter_url,
                api_key=self.config.api_key,
                model=self.config.api_model or self.config.openrouter_model
            )
        else:
            # äº‘ç«¯API (OpenAI, æ™ºè°±, é˜¿é‡Œäº‘, SiliconFlow)
            self._init_cloud_api()
        
        # åˆå§‹åŒ–é™çº§å¤‡é€‰
        if self.config.enable_fallback and self.config.fallback_to_local:
            self._init_fallback_local()
    
    def _init_local_model(self):
        """åˆå§‹åŒ–æœ¬åœ°sentence-transformersæ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨GPU"""
        try:
            from sentence_transformers import SentenceTransformer
            import torch
            import os
            
            # æ¨¡å‹ç¼“å­˜ç›®å½•ï¼ˆé¡¹ç›®æœ¬åœ°ï¼‰
            cache_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "models")
            os.makedirs(cache_dir, exist_ok=True)
            
            # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®©transformersä¹Ÿä½¿ç”¨æœ¬åœ°ç¼“å­˜
            os.environ['HF_HOME'] = cache_dir
            os.environ['TRANSFORMERS_CACHE'] = cache_dir
            
            # è®¾å¤‡é€‰æ‹©ä¼˜å…ˆçº§ï¼šcuda > mps > cpu
            device = self.config.local_device
            if device == "auto":
                print("ğŸ” æ£€æµ‹å¯ç”¨è®¾å¤‡...")
                if torch.cuda.is_available():
                    device = "cuda"
                    gpu_name = torch.cuda.get_device_name(0)
                    print(f"   âœ… GPU æ£€æµ‹åˆ°: {gpu_name}")
                    logger.info(f"GPU detected: {gpu_name}")
                elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    device = "mps"
                    print("   âœ… Apple MPS æ£€æµ‹åˆ°")
                    logger.info("Apple MPS detected")
                else:
                    device = "cpu"
                    print("   âš ï¸  æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU")
                    logger.info("No GPU detected, using CPU")
            
            print(f"\nğŸ“¥ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {self.config.local_model}")
            print(f"   ç¼“å­˜ç›®å½•: {cache_dir}")
            print(f"   è¿è¡Œè®¾å¤‡: {device}")
            print("   (é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…...)\n")
            
            logger.info(f"Loading local embedding model: {self.config.local_model}")
            logger.info(f"Model cache directory: {cache_dir}")
            logger.info(f"Using device: {device}")
            
            self._local_model = SentenceTransformer(
                self.config.local_model,
                device=device,
                cache_folder=cache_dir
            )
            
            # è·å–embeddingç»´åº¦
            self._dimension = self._local_model.get_sentence_embedding_dimension()
            print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆ! å‘é‡ç»´åº¦: {self._dimension}\n")
            logger.info(f"Local model loaded, dimension: {self._dimension}")
            
        except ImportError:
            raise ImportError(
                "sentence-transformers is required for local embedding. "
                "Install with: pip install sentence-transformers"
            )
        except Exception as e:
            logger.error(f"Failed to load local model: {e}")
            raise
    
    def _init_openai_compatible(self, base_url: str, api_key: str, model: str):
        """åˆå§‹åŒ–OpenAIå…¼å®¹APIå®¢æˆ·ç«¯"""
        try:
            from openai import OpenAI
            
            logger.info(f"Initializing OpenAI-compatible client: {base_url}")
            self._api_client = OpenAI(
                base_url=base_url,
                api_key=api_key,
                timeout=self.config.timeout
            )
            self._api_model = model
        except ImportError:
            raise ImportError(
                "openai package is required for API embedding. "
                "Install with: pip install openai"
            )
    
    def _init_cloud_api(self):
        """åˆå§‹åŒ–äº‘ç«¯APIå®¢æˆ·ç«¯"""
        base_url = self.config.get_base_url()
        model = self.config.get_effective_model()
        
        if not self.config.api_key:
            raise ValueError(
                f"API key is required for provider: {self.config.provider.value}"
            )
        
        self._init_openai_compatible(
            base_url=base_url,
            api_key=self.config.api_key,
            model=model
        )
    
    def _init_fallback_local(self):
        """åˆå§‹åŒ–é™çº§ç”¨æœ¬åœ°æ¨¡å‹"""
        if self._local_model is not None:
            # å·²ç»ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä¸éœ€è¦é™çº§
            return
        
        try:
            from sentence_transformers import SentenceTransformer
            
            logger.info(
                f"Initializing fallback local model: {self.config.fallback_local_model}"
            )
            self._fallback_model = SentenceTransformer(
                self.config.fallback_local_model
            )
        except Exception as e:
            logger.warning(f"Failed to initialize fallback model: {e}")
            self._fallback_model = None
    
    def _init_gguf_model(self):
        """åˆå§‹åŒ–GGUFé‡åŒ–æ¨¡å‹ / Initialize GGUF quantized model"""
        try:
            from llama_cpp import Llama
            import os
            
            model_path = self.config.gguf_model_path
            if not model_path:
                raise ValueError("GGUF model path is not configured")
            
            # æ”¯æŒç›¸å¯¹è·¯å¾„
            if not os.path.isabs(model_path):
                base_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
                model_path = os.path.join(base_dir, model_path)
            
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"GGUF model not found: {model_path}")
            
            n_gpu_layers = self.config.gguf_n_gpu_layers
            device_info = f"GPU (layers: {n_gpu_layers})" if n_gpu_layers != 0 else "CPU"
            
            print(f"\nğŸ“¥ æ­£åœ¨åŠ è½½GGUFåµŒå…¥æ¨¡å‹: {os.path.basename(model_path)}")
            print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
            print(f"   è¿è¡Œè®¾å¤‡: {device_info}")
            print("   (é¦–æ¬¡åŠ è½½å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ...)\n")
            
            logger.info(f"Loading GGUF embedding model: {model_path}")
            logger.info(f"GPU layers: {n_gpu_layers}")
            
            self._gguf_model = Llama(
                model_path=model_path,
                embedding=True,  # å…³é”®ï¼šå¯ç”¨åµŒå…¥æ¨¡å¼
                n_gpu_layers=n_gpu_layers,
                n_ctx=self.config.gguf_n_ctx,
                n_batch=self.config.gguf_n_batch,
                verbose=False
            )
            
            self._dimension = self.config.gguf_embedding_dim
            print(f"   âœ… GGUFæ¨¡å‹åŠ è½½å®Œæˆ! å‘é‡ç»´åº¦: {self._dimension}\n")
            logger.info(f"GGUF model loaded, dimension: {self._dimension}")
            
        except ImportError:
            raise ImportError(
                "llama-cpp-python is required for GGUF embedding. "
                "Install with: pip install llama-cpp-python\n"
                "For GPU support: CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python"
            )
        except Exception as e:
            logger.error(f"Failed to load GGUF model: {e}")
            raise
    
    def embed(self, texts: Union[str, List[str]]) -> np.ndarray:
        """
        ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡
        
        Args:
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            åµŒå…¥å‘é‡æ•°ç»„ï¼Œshapeä¸º (n_texts, dimension)
        """
        # ç¡®ä¿è¾“å…¥æ˜¯åˆ—è¡¨
        if isinstance(texts, str):
            texts = [texts]
            single_input = True
        else:
            texts = list(texts)
            single_input = False
        
        if not texts:
            return np.array([])
        
        try:
            if self._local_model is not None:
                embeddings = self._embed_local(texts)
            elif self._gguf_model is not None:
                embeddings = self._embed_gguf(texts)
            else:
                embeddings = self._embed_api(texts)
        except Exception as e:
            if self.config.enable_fallback and self._fallback_model is not None:
                logger.warning(f"Primary embedding failed, falling back to local: {e}")
                embeddings = self._embed_fallback(texts)
            else:
                raise
        
        return embeddings[0] if single_input else embeddings
    
    def _embed_local(self, texts: List[str]) -> np.ndarray:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹åµŒå…¥"""
        return self._local_model.encode(
            texts,
            batch_size=self.config.batch_size,
            show_progress_bar=len(texts) > 100,
            convert_to_numpy=True
        )
    
    def _embed_gguf(self, texts: List[str]) -> np.ndarray:
        """ä½¿ç”¨GGUFæ¨¡å‹ç”ŸæˆåµŒå…¥ / Generate embeddings using GGUF model"""
        embeddings = []
        for text in texts:
            try:
                result = self._gguf_model.create_embedding(text)
                # llama-cpp-python è¿”å›æ ¼å¼: {'data': [{'embedding': [...], 'index': 0, 'object': 'embedding'}]}
                embeddings.append(result['data'][0]['embedding'])
            except Exception as e:
                logger.error(f"GGUF embedding failed for text: {e}")
                raise
        return np.array(embeddings)
    
    def _embed_api(self, texts: List[str]) -> np.ndarray:
        """
        ä½¿ç”¨APIåµŒå…¥ - æ”¯æŒå¹¶å‘
        Embed using API - with concurrency support
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¹¶å‘
        if self.config.enable_concurrency and self.config.max_concurrency > 1:
            return self._embed_api_concurrent(texts)
        else:
            return self._embed_api_sequential(texts)
    
    def _embed_api_sequential(self, texts: List[str]) -> np.ndarray:
        """é¡ºåºå¤„ç†APIåµŒå…¥ / Sequential API embedding"""
        all_embeddings = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(texts), self.config.batch_size):
            batch = texts[i:i + self.config.batch_size]
            
            for attempt in range(self.config.max_retries):
                try:
                    response = self._api_client.embeddings.create(
                        model=self._api_model,
                        input=batch
                    )
                    batch_embeddings = [d.embedding for d in response.data]
                    all_embeddings.extend(batch_embeddings)
                    break
                except Exception as e:
                    if attempt == self.config.max_retries - 1:
                        raise
                    logger.warning(
                        f"API embedding attempt {attempt + 1} failed: {e}, retrying..."
                    )
        
        return np.array(all_embeddings)
    
    def _embed_api_concurrent(self, texts: List[str]) -> np.ndarray:
        """
        å¹¶å‘å¤„ç†APIåµŒå…¥ / Concurrent API embedding
        
        ä½¿ç”¨çº¿ç¨‹æ± å®ç°å¹¶å‘ï¼Œæé«˜å¤„ç†é€Ÿåº¦
        """
        import concurrent.futures
        
        # åˆ†æ‰¹
        batches = []
        for i in range(0, len(texts), self.config.batch_size):
            batches.append(texts[i:i + self.config.batch_size])
        
        all_embeddings = [None] * len(batches)
        max_workers = min(self.config.max_concurrency, len(batches))
        
        def embed_batch(batch_idx: int, batch: List[str]) -> tuple:
            """åµŒå…¥å•ä¸ªæ‰¹æ¬¡"""
            for attempt in range(self.config.max_retries):
                try:
                    response = self._api_client.embeddings.create(
                        model=self._api_model,
                        input=batch
                    )
                    embeddings = [d.embedding for d in response.data]
                    return batch_idx, embeddings
                except Exception as e:
                    if attempt == self.config.max_retries - 1:
                        logger.error(f"Batch {batch_idx} failed after {self.config.max_retries} attempts: {e}")
                        raise
                    logger.warning(f"Batch {batch_idx} attempt {attempt + 1} failed: {e}, retrying...")
            return batch_idx, []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(embed_batch, idx, batch): idx 
                for idx, batch in enumerate(batches)
            }
            
            for future in concurrent.futures.as_completed(futures):
                try:
                    batch_idx, embeddings = future.result()
                    all_embeddings[batch_idx] = embeddings
                except Exception as e:
                    logger.error(f"Concurrent embedding failed: {e}")
                    raise
        
        # åˆå¹¶ç»“æœ
        final_embeddings = []
        for batch_embeddings in all_embeddings:
            if batch_embeddings:
                final_embeddings.extend(batch_embeddings)
        
        return np.array(final_embeddings)
    
    def _embed_fallback(self, texts: List[str]) -> np.ndarray:
        """ä½¿ç”¨é™çº§æ¨¡å‹åµŒå…¥"""
        return self._fallback_model.encode(
            texts,
            batch_size=self.config.batch_size,
            show_progress_bar=len(texts) > 100,
            convert_to_numpy=True
        )
    
    @property
    def dimension(self) -> Optional[int]:
        """è·å–åµŒå…¥å‘é‡ç»´åº¦"""
        if self._dimension is not None:
            return self._dimension
        
        # å¦‚æœä½¿ç”¨APIï¼Œé€šè¿‡æµ‹è¯•è·å–ç»´åº¦
        if self._api_client is not None:
            try:
                test_embedding = self.embed("test")
                self._dimension = len(test_embedding)
                return self._dimension
            except:
                pass
        
        return None
    
    @property
    def provider_name(self) -> str:
        """è·å–å½“å‰æä¾›å•†åç§°"""
        return self.config.provider.value
    
    @property
    def model_name(self) -> str:
        """è·å–å½“å‰æ¨¡å‹åç§°"""
        return self.config.get_effective_model()
